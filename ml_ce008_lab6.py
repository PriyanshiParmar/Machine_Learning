# -*- coding: utf-8 -*-
"""ML_CE008_Lab6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UbwsmbEONQYUgoBJvJPYYDW4qwKHb8hA
"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
import string
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import classification_report
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# nltk.download('stopwords')

columns = ["sentiment", "id", "date", "query", "user_id", "text"]
df = pd.read_csv("drive/MyDrive/training.1600000.processed.noemoticon.csv", encoding="latin",names=columns)
# print(df.head())
df["sentiment"] = df["sentiment"].replace(4,1)
sns.countplot(x="sentiment",data=df)

positive_tweets = df[df['sentiment'] == 1]['text'].tolist()[6996:7996]
negative_tweets = df[df['sentiment'] == 0]['text'].tolist()[6996:7996]

# print(type(positive_tweets_string))
# positive_tweets_string = " ".join(str(v) for v in positive_tweets)
# print(positive_tweets_string)
# plt.figure(figsize=(15,15))
# plt.imshow(WordCloud().generate(positive_tweets_string))

# negative_tweets = df[df['sentiment'] == 0]['text'].tolist()
# # negative_tweets = negative_tweets.iloc[10000: , :]
# negative_tweets = pd.DataFrame(negative_tweets)
# print(negative_tweets.shape)
# negative_tweets = negative_tweets.iloc[50000: , :]

# negative_tweets = negative_tweets.values.tolist()
# print(len(negative_tweets))
# negative_tweets_string = " ".join(str(v) for v in negative_tweets) 
# print(negative_tweets_string)
# plt.figure(figsize=(15,15))
# plt.imshow(WordCloud().generate(negative_tweets_string))

from google.colab import drive
drive.mount('/content/drive')

print(type(negative_tweets[37]))
print(''.join(negative_tweets[37]))
print(''.join(positive_tweets[3]))
positive_tweets[3]

import nltk
import numpy
import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import TweetTokenizer
#Remove stopwords
nltk.download('stopwords')

import re
stopwords_english = stopwords.words('english')
# Now using all the knowledge, creating clean positive and negative tweets

# Step 1 - Remove hyperlinks and hashtags
for i in range(len(positive_tweets)):
  positive_tweets[i] = re.sub(r'''(https?:\/\/)(\s)*(www\.)?(\s)*((\w|\s)+\.)*([\w\-\s]+\/)*([\w\-]+)((\?)?[\w\s]*=\s*[\w\%&]*)*''','',positive_tweets[i]);
  positive_tweets[i] = re.sub(r'#','',positive_tweets[i])
  negative_tweets[i] = re.sub(r'''(https?:\/\/)(\s)*(www\.)?(\s)*((\w|\s)+\.)*([\w\-\s]+\/)*([\w\-]+)((\?)?[\w\s]*=\s*[\w\%&]*)*''','',negative_tweets[i]);
  negative_tweets[i] = re.sub(r'#','',negative_tweets[i])

# Step 2 - Tokenize the string

tokenizer = TweetTokenizer(preserve_case=False)

for i in range(len(positive_tweets)):
  positive_tweets[i] = tokenizer.tokenize(positive_tweets[i])
  negative_tweets[i] = tokenizer.tokenize(negative_tweets[i])

# Step 3 - Remove the stopwords and punctuations
for i in range(len(positive_tweets)):
  temp_pos = [*positive_tweets[i]]
  positive_tweets[i] = []
  for word in temp_pos:
    if (word not in stopwords_english and word not in string.punctuation):
      positive_tweets[i].append(word)

  temp_neg = [*negative_tweets[i]]
  negative_tweets[i] = []
  for word in temp_neg:
    if (word not in stopwords_english and word not in string.punctuation):
      negative_tweets[i].append(word)

positive_tweets_storage = positive_tweets
negative_tweets_storage = negative_tweets

positive_tweets[0], negative_tweets[0]
# pos_tweets_clean = positive_tweets
# neg_tweets_clean = negative_tweets

# #Tokenize
# tokenizer = TweetTokenizer(preserve_case=False)
# for i in range(len(positive_tweets)):
#   positive_tweets[i] = tokenizer.tokenize(positive_tweets[i])
# for i in range(len(negative_tweets)):
#   negative_tweets[i] = tokenizer.tokenize(negative_tweets[i])

# # print(negative_tweets[37])

# for i in range(len(positive_tweets)):
#   pos = [*positive_tweets[i]]
#   pos_tweets_clean[i] = []
#   for word in pos:
#     if(word not in stopwords_english) and (word not in string.punctuation):
#       pos_tweets_clean[i].append(word)

# # print(negative_tweets[37])

# for i in range(len(negative_tweets)):
#   neg = [*negative_tweets[i]]
#   neg_tweets_clean[i] = []
#   for word in neg:
#     if(word not in stopwords_english) and (word not in string.punctuation):
#       neg_tweets_clean[i].append(word)

# # print(negative_tweets[37])

# print("Positive tweet: ",pos_tweets_clean[79])
# print(len(pos_tweets_clean))
# print("Negative tweet: ",neg_tweets_clean[67])
# print(len(neg_tweets_clean))

# pos_tweets_clean[3]
# print(negative_tweets[37])

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')

# Lemmatizing all the tweets
from nltk.stem import WordNetLemmatizer
print(positive_tweets[10][2])

for i in range(len(positive_tweets)):
  for j in range(len(positive_tweets[i])):
    # print("here->",positive_tweets[i][j])
    positive_tweets[i][j] = WordNetLemmatizer().lemmatize(positive_tweets[i][j],'v')

for i in range(len(negative_tweets)):
  for j in range(len(negative_tweets[i])):
    negative_tweets[i][j] = WordNetLemmatizer().lemmatize(negative_tweets[i][j],'v')

positive_tweets[0], negative_tweets[0]

positive_text = []
negative_text = []
for wordlist in positive_tweets:
  positive_text = [*positive_text, " ".join(wordlist)]

for wordlist in negative_tweets:
  negative_text = [*negative_text, " ".join(wordlist)]

# pos_tweets_stem[9][1]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

vectorizer = CountVectorizer()

# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

data = []
data_labels = []

for word in positive_text:
  data.append(word)
  data_labels.append(1)

for word in negative_text:
  data.append(word)
  data_labels.append(0)

features = vectorizer.fit_transform(data)
features_nd = features.toarray()

# positive_text[13]

# data = []
# labels = []

# for word in positive_text:
#   data.append(word)
#   labels.append(1)

# # print(len(data))

# for word in negative_text:
#   data.append(word)
#   labels.append(0)

# data[249]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    features_nd,
    data_labels,
    train_size = 0.85,
    random_state = 81
)

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(features_nd, data_labels)

y_pred = model.predict(X_test)
from sklearn import metrics

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test,y_pred))
print("Recall:",metrics.recall_score(y_test,y_pred))
print("F1 score:",metrics.f1_score(y_test,y_pred))

"""Implementing Decision Tree

"""

from sklearn import tree

dt = tree.DecisionTreeClassifier()
dt = dt.fit(X_train, y_train)

dt_test_pred = dt.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, dt_test_pred))
print("Precision:",metrics.precision_score(y_test,dt_test_pred))
print("Recall:",metrics.recall_score(y_test,dt_test_pred))
print("F1 score:",metrics.f1_score(y_test,dt_test_pred))