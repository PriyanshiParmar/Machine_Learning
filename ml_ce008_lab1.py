# -*- coding: utf-8 -*-
"""ML_CE008_Lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fuczHSTlxZTXB1qnBbwl_Rbw57WqbKjv
"""

#3.1.1
#1
#import numpy as np

x = 5
print(x," Type(x): ",type(x));
x = "Hello"
print(x," Type(x): ",type(x));
x = [1,2,3]
print(x," Type(x): ",type(x));

#3.1.1
#2

import numpy as np
alist = [1,2,3,4,5]
narray = np.array([1,2,3,4])
print(alist," Type(alist): ",type(alist));
print(narray," Type(narray): ",type(narray));

#3.1.2
import numpy as np
alist = [1,2,3,4,5]
narray = np.array([1,2,3,4])
print(narray + narray)
narray1 = np.array([1,2,3])
# print(narray + narray1)
print(alist + alist)
alist1 = [1,2,3,4]
print(alist + alist1)
print(narray * 3)
print(alist * 3)

x = np.array([[1,2],[3,4]], dtype=np.float64)
y = np.array([[5,6],[7,8]], dtype=np.float64)
# Elementwise sum; both produce the array
print(x + y)
print(np.add(x, y))

# Elementwise difference; both produce the array
print(x - y)
print(np.subtract(x, y))
# Elementwise product; both produce the array
print(x * y)
print(np.multiply(x, y))
print(x.dot(y))

# Elementwise division; both produce the array
# [[ 0.2 0.33333333]
# [ 0.42857143 0.5 ]]
print(x / y)
print(np.divide(x, y))
# Elementwise square root; produces the array
# [[ 1. 1.41421356]
# [ 1.73205081 2. ]]
print(np.sqrt(x))
x = np.array([[1,2],[3,4]])
y = np.array([[5,6],[7,8]])
v = np.array([9,10])
w = np.array([11, 12])
# Inner product of vectors; both produce 219
print(v.dot(w))
print(np.dot(v, w))

print(v @ w)
# Matrix / vector product; both produce the rank 1 array [29 67]
print(x.dot(v))
print(np.dot(x, v))
print(x @ v)
# Matrix / matrix product; both produce the rank 2 array
# [[19 22]
# [43 50]]
print(x.dot(y))
print(np.dot(x, y))
print(x @ y)

import numpy as np
x = np.array([[1,2],[3,4]])
print(np.sum(x)) # Compute sum of all elements; prints "10"
print(np.sum(x, axis=0)) # Compute sum of each column; prints "[4 6]"
print(np.sum(x, axis=1)) # Compute sum of each row; prints "[3 7]"
print(x)
print("transpose\n", x.T)

import numpy as np
npmatrix1 = np.array([narray, narray, narray]) # Matrix initialized with NumPy␣,→arrays
npmatrix2 = np.array([alist, alist, alist]) # Matrix initialized with lists
npmatrix3 = np.array([narray, [1, 1, 1, 1], narray]) # Matrix initialized with␣,→both types
print(npmatrix1)

print(npmatrix2)
print(npmatrix3)

# Example 1:
okmatrix = np.array([[1, 2], [3, 4]]) # Define a 2x2 matrix
print(okmatrix) # Print okmatrix
print(okmatrix * 2) # Print a scaled version of okmatrix

# Example 2:
badmatrix = np.array([[1,2 ], [3, 4], [6,7, 8]], dtype=object) # Define a matrix.,→ Note the third row contains 3 elements
print(badmatrix) # Print the malformed matrix
print(badmatrix * 2) # It is supposed to scale the whole matrix

result = okmatrix * 2 + 1 # For each element in the matrix, multiply by 2 and␣,→add 1
print(result)

# Add two sum compatible matrices
result1 = okmatrix + okmatrix
print(result1)
# Subtract two sum compatible matrices. This is called the difference vector
result2 = okmatrix - okmatrix
print(result2)

result = okmatrix * okmatrix # Multiply each element by itself
print(result)

matrix3x2 = np.array([[1, 2], [3, 4], [5, 6]]) # Define a 3x2 matrix
print('Original matrix 3 x 2')
print(matrix3x2)
print('Transposed matrix 2 x 3')
print(matrix3x2.T)

nparray = np.array([1, 2, 3, 4]) # Define an array
print('Original array')
print(nparray)
print('Transposed array')
print(nparray.T)

nparray = np.array([[1, 2, 3, 4]]) # Define a 1 x 4 matrix. Note the 2 level of␣square brackets
print('Original array')
print(nparray)
print('Transposed array')
print(nparray.T)

nparray1 = np.array([1, 2, 3, 4]) # Define an array
norm1 = np.linalg.norm(nparray1)
nparray2 = np.array([[1, 2], [3, 4]]) # Define a 2 x 2 matrix. Note the 2 level␣of square brackets
norm2 = np.linalg.norm(nparray2)
print(norm1)
print(norm2)

nparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix.
normByCols = np.linalg.norm(nparray2, axis=0) # Get the norm for each column.␣Returns 2 elements
normByRows = np.linalg.norm(nparray2, axis=1) # get the norm for each row.␣Returns 3 elements
print(normByCols)
print(normByRows)

nparray1 = np.array([0, 1, 2, 3]) # Define an array
nparray2 = np.array([4, 5, 6, 7]) # Define an array
flavor1 = np.dot(nparray1, nparray2) # Way-1
print(flavor1)
flavor2 = np.sum(nparray1 * nparray2) # Way-2
print(flavor2)
flavor3 = nparray1 @ nparray2 # Way-3
print(flavor3)
# As you never should do: #Way-4
flavor4 = 0
for a, b in zip(nparray1, nparray2):
  flavor4 += a * b
print(flavor4)

norm1 = np.dot(np.array([1, 2]), np.array([3, 4])) # Dot product on nparrays
norm2 = np.dot([1, 2], [3, 4]) # Dot product on python lists
print(norm1, '=', norm2 )

nparray2 = np.array([[1, -1], [2, -2], [3, -3]]) # Define a 3 x 2 matrix.
sumByCols = np.sum(nparray2, axis=0) # Get the sum for each column. Returns 2␣elements
sumByRows = np.sum(nparray2, axis=1) # get the sum for each row. Returns 3␣elements
print('Sum by columns: ')
print(sumByCols)
print('Sum by rows:')
print(sumByRows)

nparray2 = np.array([[1, -1], [2, -2], [3, -3]]) # Define a 3 x 2 matrix. Chosen␣to be a matrix with 0 mean
mean = np.mean(nparray2) # Get the mean for the whole matrix
meanByCols = np.mean(nparray2, axis=0) # Get the mean for each column. Returns 2␣elements
meanByRows = np.mean(nparray2, axis=1) # get the mean for each row. Returns 3␣elements
print('Matrix mean: ')
print(mean)
print('Mean by columns: ')
print(meanByCols)
print('Mean by rows:')
print(meanByRows)

nparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix.
nparrayCentered = nparray2 - np.mean(nparray2, axis=0) # Remove the mean for␣each column
print('Original matrix')
print(nparray2)
print('Centered by columns matrix')
print(nparrayCentered)
print('New mean by column')
print(nparrayCentered.mean(axis=0))

nparray2 = np.array([[1, 3], [2, 4], [3, 5]]) # Define a 3 x 2 matrix.
nparrayCentered = nparray2.T - np.mean(nparray2, axis=1) # Remove the mean for␣each row
nparrayCentered = nparrayCentered.T # Transpose back the result
print('Original matrix')
print(nparray2)
print('Centered by columns matrix')
print(nparrayCentered)
print('New mean by rows')
print(nparrayCentered.mean(axis=1))

import nltk # Python library for NLP
from nltk.corpus import twitter_samples # sample Twitter dataset from NLTK
import matplotlib.pyplot as plt # library for visualization
import random

#downloads sample twitter dataset.
nltk.download('twitter_samples')

# select the set of positive and negative tweets
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

print('Number of positive tweets: ', len(all_positive_tweets))
print('Number of negative tweets: ', len(all_negative_tweets))
print('\nThe type of all_positive_tweets is: ', type(all_positive_tweets))
print('The type of a tweet entry is: ', type(all_negative_tweets[0]))

# Declare a figure with a custom size
fig = plt.figure(figsize=(5, 5))
# labels for the classes
labels = 'ML-HAP-Lec', 'ML-SPS-Lec','ML-HAP-Lab','ML-SPS-Lab'
# Sizes for each slide
sizes = [35, 35, 15,15]
# Declare pie chart, where the slices will be ordered and plotted␣counter-clockwise:
plt.pie(sizes, labels=labels, autopct='%.2f%%',
shadow=True, startangle=90)
#autopct enables you to display the percent value using Python string formatting.
#For example, if autopct='%.2f', then for each pie wedge, the format string is␣'%.2f' and

# Equal aspect ratio ensures that pie is drawn as a circle.
plt.axis('equal')
# Display the chart
plt.show()

# Declare a figure with a custom size
fig = plt.figure(figsize=(5, 5))
# labels for the two classes
labels = 'Positives', 'Negative'
# Sizes for each slide
sizes = [len(all_positive_tweets), len(all_negative_tweets)]
# Declare pie chart, where the slices will be ordered and plotted␣counter-clockwise:
plt.pie(sizes, labels=labels, autopct='%1.1f%%',
shadow=True, startangle=90)
# Equal aspect ratio ensures that pie is drawn as a circle.
plt.axis('equal')
# Display the chart
plt.show()

# print positive in greeen
print('\033[92m' + all_positive_tweets[random.randint(0,5000)])
# print negative in red
print('\033[91m' + all_negative_tweets[random.randint(0,5000)])
# Our selected sample
tweet = all_positive_tweets[2277]
print(tweet)
# download the stopwords from NLTK
nltk.download('stopwords')

import re # library for regular expression␣operations
import string # for string operations
from nltk.corpus import stopwords # module for stop words that come␣with NLTK
from nltk.stem import PorterStemmer # module for stemming
from nltk.tokenize import TweetTokenizer # module for tokenizing strings
from nltk.stem import WordNetLemmatizer

print('\033[92m' + tweet)
print('\033[94m')
# remove hyperlinks
tweet2 = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)
# remove hashtags
# only removing the hash # sign from the word
tweet2 = re.sub(r'#', '', tweet2)
print(tweet2)

print()
print('\033[92m' + tweet2)
print('\033[94m')
# instantiate tokenizer class
tokenizer = TweetTokenizer(preserve_case=False)
# tokenize tweets
tweet_tokens = tokenizer.tokenize(tweet2)
print()
print('Tokenized string:')
print(tweet_tokens)

#Import the english stop words list from NLTK
stopwords_english = stopwords.words('english')
print('Stop words\n')
print(stopwords_english)
print('\nPunctuation\n')
print(string.punctuation)

print()
print('\033[92m')
print(tweet_tokens)
print('\033[94m')
tweets_clean = []
for word in tweet_tokens: # Go through every word in your tokens list
  if (word not in stopwords_english and word not in string.punctuation): # remove punctuation
    tweets_clean.append(word)
print('removed stop words and punctuation:')
print(tweets_clean)

words =['happier','happiness','happy','studying','study','studies','meeting']
print()
print('\033[92m')
print(tweets_clean)
print('\033[94m')
# Instantiate stemming class
stemmer = PorterStemmer()
# Create an empty list to store the stems
tweets_stem = []
for word in tweets_clean:
  stem_word = stemmer.stem(word) # stemming word
tweets_stem.append(stem_word) # append to the list
print('stemmed words:')
print(tweets_stem)
print()
print('\033[92m')
print(tweets_clean)
print('\033[94m')

# Instantiate stemming class
stemmer = PorterStemmer()
# Create an empty list to store the stems
tweets_stem = []
for word in words:
  stem_word = stemmer.stem(word) # stemming word
  tweets_stem.append(stem_word) # append to the list
print('stemmed words:')
print(tweets_stem)

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
print()
print('\033[92m')
print(tweets_clean)
print('\033[94m')
# Instantiate stemming class
wordnet_lemmatizer = WordNetLemmatizer()
# Create an empty list to store the stems
tweets_lem = []
for word in tweets_clean:
  lem_words = wordnet_lemmatizer.lemmatize(word)# stemming word
  tweets_lem.append(lem_words) # append to the list
print('lemmatized words:')
print(tweets_lem)

print()
print('\033[92m')
print(tweets_clean)
print('\033[94m')
# Instantiate stemming class
wordnet_lemmatizer = WordNetLemmatizer()
# Create an empty list to store the stems
tweets_lem = []
for word in words:
  lem_words = wordnet_lemmatizer.lemmatize(word,pos ="v")# stemming word
  tweets_lem.append(lem_words) # append to the list
print('lemmatized words:')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

data=pd.read_csv('drive/MyDrive/mtcars.csv')
d=pd.crosstab(index=data['cyl'],columns="count",dropna=True)
print(d)
data.head()
data.tail()
data.describe()
data.info()

#Count Total Null values in each column
print("Total Null Data:",data.isnull().sum())
plt.hist(data['mpg'],bins=5)
plt.show()
plt.scatter(data['mpg'],data['wt'])
plt.show()

df=pd.DataFrame(data,columns=['gear'])
print("Count How many values:\n",df['gear'].value_counts())

data.values

data.loc[5:,'mpg']

# selecting range of rows from 2 to 5
display(data.loc[2 : 5])

# selecting rows from 1 to 4 and columns from 2 to 4
display(data.iloc[1 : 5, 2 : 5])

import numpy as np
import matplotlib as plt
from sklearn import datasets #various toy datasets
from sklearn import metrics #Check accuracy of model
from sklearn.linear_model import LogisticRegression

iris = datasets.load_iris()
X = iris.data
y= iris.target
feature_names = iris.feature_names
target_names = iris.target_names
print("Feature names:", feature_names)
print("Target names:", target_names)

print("\nFirst 10 rows of X:\n", X[:10])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.4, random_state=1)

"""Exercise

1) Create Two numpy array of size 4 X 5 and 5 X 4.
"""

import numpy as np
#arr1 = np.array([[1,2,3,4,5],[2,3,4,5,6],[3,5,7,9,11],[8,12,14,15,20]])
#arr2 = np.array([[1,2,3,4],[2,4,6,8],[3,5,7,9],[10,12,14,16],[20,21,22,23]])
arr1 = np.empty([4,5])
arr2 = np.empty([5,4])
print(arr1)
print(arr2)
print(type(arr1));
print(type(arr2));

"""2) Randomly Initalize that array."""

arr1 = np.random.randint(5,size=(4,5))
arr2 = np.random.randint(5, size=(5,4))
print(arr1)
print(arr2)
print(type(arr1));
print(type(arr2));

"""3) Perform matrix multiplication"""

multipliedMatrix = arr1 @arr2
print(multipliedMatrix)
print(type(multipliedMatrix))

"""4) Perform elementwise matrix multiplication"""

# arr1 = np.array([[1,2,3,4,5],[2,3,4,5,6],[3,5,7,9,11],[8,12,14,15,20]])
# arr2 = np.array([[1,2,3,4,5],[2,3,4,5,6],[3,5,7,9,11],[8,12,14,15,20]])
elementwiseMultiplication = arr1 * arr1
print(elementwiseMultiplication)
# print(type(elementwiseMultiplication))

"""5) Find mean, median of the first matrix."""

print(arr1)
print("Mean: ",np.mean(arr1))
print("Median: ",np.median(arr1))

"""6) Get the transpose of that Matrix that you created. Create a square matrix and find its deter-
minant.
"""

newArr = arr1.T
print(arr1)
print(newArr)

"""7) Obtain each row in the second column of the first array."""

for i in range(4):
  print(arr1[i][1])

"""8) Convert Numeric entries(columns) of mtcars.csv to Mean Centered Version"""

import pandas as pd
import numpy as np

data = []
csvFile = pd.read_csv("drive/MyDrive/mtcars.csv");
for lines in csvFile:
  data.append(lines)
# the heading row is not useful
data = data[1:]
data = np.array(data)
data = np.delete(data, 0)

data = data.astype(float)
mean = np.mean(data, axis=0)
print(data)
for i in range(len(data)):
    data[i] = data[i] - mean
print(data)

"""NLTK Library

Perform all text preprocessing tasks on Movie Review Dataset
"""

import pandas as pd
import numpy as np
import nltk
import re
nltk.download('punkt')
from nltk.corpus import stopwords
stopword = stopwords.words('english')
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
import xgboost as xgb
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import CountVectorizer 

data = pd.read_csv("drive/MyDrive/IMDB Dataset.csv")
n = len(data)

lemmatizer = WordNetLemmatizer()

corpus = []
for i in range(n):
    review = data['review'][i]
    review = re.sub('[^a-zA-Z0-9]' , ' ' , review)#Removing Special Characters and digits
    review = word_tokenize(review)#Word Tokenization
    review = [word for word in review if word not in stopword]
    review = [lemmatizer.lemmatize(word) for word in review ]
    review = [word.lower() for word in review] #Keep everything in lower case
    review = ' '.join(review)
    corpus.append(review)

"""Pandas Library
1. Draw Scatter Plot between SepalLengthCm and SepalWidthCm for “Iris.csv” file with proper labeling.
2. Draw Histogram of SepalLengthCm with proper labeling.
3. Plot bar chart of Species.
4. Count total null values for each column in this dataset.
5. i) Print first 5 rows of SepalLengthCm. ii) Print from 5th row onwards all columns of Iris.csv dataset.
"""

import pandas as pd
data = pd.read_csv('drive/MyDrive/Iris.csv')
plt.scatter(data['SepalLengthCm'], data['SepalWidthCm'])
plt.show()

plt.hist(data['SepalLengthCm'],bins=5)
plt.show

data.head()
df = pd.DataFrame(np.random.rand(10, 5),
     columns =['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'veriety'])
df.plot.bar()

print("Total Null Values:",data.isnull().sum())

data.head()
display(data.iloc[1:6, :1])
display(data.iloc[5: , :])

"""Exercise for Scikit Learn Library: Perform the mentioned steps of the Implementation Guidelines
on Breast Cancer Dataset of Scikit Learn Library.
"""

from sklearn.datasets import load_breast_cancer
breast_cancer = load_breast_cancer()

x = breast_cancer.data
y = breast_cancer.target

feature_names = breast_cancer.feature_names
target_names = breast_cancer.target_names
print("Feature names:", feature_names)
print("Target names:", target_names)
print("\nFirst 10 rows of X:\n", x[:10])

"""5 Exercise 1.1

2. Write the function to generate random numbers from a normal distribution with mean 0 and
variance 1.
"""



"""3. Create a 1D array arr1 with 10 elements.
(a) Slice this array starting from 3rd value to the end.
(b) What is the result if you perform the following slicing on the given array. arr1[-4:-2]
"""

#3 
arr1 = [1,2,3,4,5,6,7,8,9,10]
print(arr1[2:])
arr1[-4:-2]

"""Create two numpy arrays of size 4 x 4. Write functions used to calculate determinant of
matrix, transpose of a matrix, norm of a matrix and to perform matrix multiplication. Write
all the results.
"""

import numpy as np
import pandas as pd


M1 = np.array([[1,5,-3,4], 
    [5,6,5,8],
    [9,25,-11,56],
    [13,14,65,16]])
    
M2 = np.array([[16,15,-14,13], 
    [12,22,15,9],
    [8,75,56,5],
    [-4,-3,2,1]])
    
print(np.linalg.det(M1))
print(np.linalg.det(M2))

print(M1.T)
print(M2.T)

print(np.linalg.norm(M1))
print(np.linalg.norm(M2))

print(M1*M2)

import nltk # Python library for NLP
from nltk.corpus import twitter_samples # sample Twitter dataset from NLTK
import matplotlib.pyplot as plt # library for visualization
import random # pseudo-random number generator
import re # library for regular expression operations
import string # for string operations
from nltk.corpus import stopwords # module for stop words that come with NLTK
from nltk.stem import PorterStemmer # module for stemming
from nltk.tokenize import TweetTokenizer # module for tokenizing strings
from nltk.stem import WordNetLemmatizer
import re

#downloads sample twitter dataset.
nltk.download('twitter_samples')

# download the stopwords from NLTK
nltk.download('stopwords')

nltk.download('wordnet')
nltk.download('omw-1.4')

# select the set of positive and negative tweets
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

tweet = all_positive_tweets[2]

print('\033[94m')
# remove hyperlinks
tweet2 = re.sub(r'https?:\/\/.[\r\n]', '', tweet)
# remove hashtags
# only removing the hash # sign from the word
tweet2 = re.sub(r'#', '', tweet2)
# only removing the hash @ sign from the word
tweet2 = re.sub(r'@', '', tweet2)
print(tweet2)

# instantiate tokenizer class
tokenizer = TweetTokenizer(preserve_case=False)
# tokenize tweets
tweet_tokens = tokenizer.tokenize(tweet2)

print()
print('Tokenized string:')
print(tweet_tokens)

#Import the english stop words list from NLTK
stopwords_english = stopwords.words('english')

print('\033[92m')
tweets_clean = []
tweets_clean1 = []
for word in tweet_tokens: # Go through every word in your tokens list
  if (word not in stopwords_english and # remove stopwords
    word not in string.punctuation): # remove punctuation
      tweets_clean.append(word)
      tweets_clean1.append(word)
print('removed stop words and punctuation:')
print(tweets_clean)


# Instantiate stemming class
stemmer = PorterStemmer()
# Create an empty list to store the stems
tweets_stem = []
for word in tweets_clean:
  stem_word = stemmer.stem(word) # stemming word
  tweets_stem.append(stem_word) # append to the list
print('stemmed words:')
print(tweets_stem)

# Instantiate stemming class
wordnet_lemmatizer = WordNetLemmatizer()
# Create an empty list to store the stems
tweets_lem = []
for word in tweets_clean1:
  lem_words = wordnet_lemmatizer.lemmatize(word)# stemming word
  tweets_lem.append(lem_words) # append to the list
print('lemmatized words:')

print(tweets_lem)

"""Describe the dataset used for Pandas Lab exercise Task. Write the function used to select
rows from 1 to 4 and columns from 2 to 4 for iris dataset. Write the results.
"""

import numpy as np
import pandas as pd
from sklearn import datasets #various toy datasets

iris = datasets.load_iris()
x = iris.data
print(x[1:5,2:])

"""8. Describe the dataset used for sklearn lab exercise task. Write the function used to divide the
dataset into training and testing and descibe its parameters.
"""

from sklearn.model_selection import train_test_split
from sklearn import datasets #various toy datasets

data = datasets.load_breast_cancer()

X = data.data
y= data.target

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size = 0.4, random_state=1
)