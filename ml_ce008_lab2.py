# -*- coding: utf-8 -*-
"""ML_CE008_Lab2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T_UEy-FRS12uSfTonw522rIVPLFuJjHQ
"""

# Step 1: Import Libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Step 2: Load Data
datasets = pd.read_csv('drive/MyDrive/Data_for_Transformation.csv')
print("\nData :\n",datasets)
#print("\nData statistics\n",datasets.describe())
# Step 3: Seprate Input and Output attributes
# All rows, all columns except last
X = datasets.iloc[:, :-1].values
print("\n\nX for transformation : \n", X)
# Only last column
Y = datasets.iloc[:, -1].values
print("\n\nY for transformation : \n", Y)
#print("\n\nInput : \n", X)
#print("\n\nOutput: \n", Y)
X_new = datasets.iloc[:,1:3].values
print("\n\nX_new for transformation : \n", X_new)

# Step 4 : Perform scaling on age and salary
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_new)
print("\n\nScaled X : \n", X_scaled)

# Step 5 : Perform standardization on age and salary
std = StandardScaler()
X_std = std.fit_transform(X_new)
print("\n\nStandardized X : \n", X_std)

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

datasets = pd.read_csv('drive/MyDrive/Data_for_Categorical_Values.csv')
print("\nData :\n", datasets)
print("\nDatasets: \n", datasets.describe())

X = datasets.iloc[:, :-1].values
Y = datasets.iloc[:, -1].values

print("\n\nInput: \n", X)
print("\n\nOutput: \n", Y)

le = LabelEncoder()
X[ : , 0] = le.fit_transform(X[ : , 0])
print("\n\nInput: \n", X)

# Step 4b: Use dummy variables from pandas library
# to create one column for each country
dummy = pd.get_dummies(datasets['Country'])
print("\n\nDummy :\n",dummy)
datasets = datasets.drop(['Country','Purchased'],axis=1)
datasets = pd.concat([dummy,datasets],axis=1)
print("\n\nFinal Data :\n",datasets)

#Use One Hot Encoder from scikit learn
onehotencoder = OneHotEncoder()
#reshape the 1-D country array to 2-D as fit_transform expects 2-D and finally fit the‚ê£object
x = onehotencoder.fit_transform(datasets.Country.values.reshape(-1,1)).toarray()

x

dfOneHot = pd.DataFrame(x, columns = ["Country_"+str(int(i)) for i in range(datasets.shape[1]-1)])
df = pd.concat([datasets, dfOneHot], axis=1) #column
#droping the country column
df= df.drop(['Country'], axis=1)
#printing to verify
print(df.head())

"""4.3 Steps for Handling the missing value"""

# Step 1: Import Libraries
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
# Step 2: Load Data
datasets = pd.read_csv('drive/MyDrive/Data_for_Missing_Values.csv')
print("\nData :\n",datasets)
print("\nData statistics\n",datasets.describe())

# Step 3: Seprate Input and Output attributes
# All rows, all columns except last
X = datasets.iloc[:, :-1].values
# Only last column
Y = datasets.iloc[:, -1].values
print("\n\nInput : \n", X)
print("\n\nOutput: \n", Y)

# Step 4: Find the missing values and handle it in either way
# 4a. Removing the row with all null values
datasets.dropna(axis=0,how='all',inplace=True)
print("\nNew Data :",datasets)

#4b. Removing the row with any one null values
#datasets.dropna(axis=0,how='any',inplace=True)

updated_df = datasets;
updated_df['Age']=updated_df['Age'].fillna(updated_df['Age'].mean())
updated_df.info()

datasets

updated_df = datasets;
updated_df['Salary']=updated_df['Salary'].fillna(updated_df['Salary'].mean())
updated_df.info()

datasets

new_X = datasets.iloc[:, :-1].values
# Only last column
new_Y = datasets.iloc[:, -1].values
print(new_X)
print(new_Y)

"""4.4 Correlation"""

import numpy as np
import pandas as pd
import seaborn as sns

data = pd.read_csv('drive/MyDrive/Data_for_Correlation.csv')
data.head()

data = data.iloc[:,:-1]
data.head()

data.info()

corr = data.corr()
corr.head()

sns.heatmap(corr)

"""Exercise

1) Perform all data preprocessing tasks and feature selection on "Exercise-CarData.csv" Hint: Check
"na_values" attribute of pd.read()
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler


datasets = pd.read_csv('drive/MyDrive/Exercise-CarData.csv')
print(datasets.head())

print(datasets.info())
datasets.shape

from sklearn.preprocessing import MinMaxScaler, StandardScaler

X_new = datasets.iloc[:,1:3].values
# MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_new)
print(X_scaled)

# standardscaler
std = StandardScaler()
X_std = std.fit_transform(X_new)
print(X_std)

from sklearn.preprocessing import LabelEncoder,OneHotEncoder

le = LabelEncoder()
X = datasets.iloc[:, :5].values
X[: ,4] = le.fit_transform(X[ : ,4])
print(X)

dm = pd.get_dummies(datasets['FuelType'])
print("\n\nDummy :\n",dm)
datasets = datasets.drop(['FuelType'],axis=1)
data = pd.concat([dm,datasets],axis=1)
print(data)

#Seprate Input and Output attributes
# All rows, all columns except last
# X = data.iloc[:, :-1].values
# Only last column
# Y = data.iloc[:, -1].values
#print("\n\nInput : \n", X)
#print("\n\nOutput: \n", Y)
X_new = data.iloc[:,1:3].values
# print("\n\nX for transformation : \n", X_new)

#Remove row with all null values
data['Age'] = data['Age'].fillna(data['Age'].mean())
data.dropna(axis=0, how='any', inplace=True)
print(data) 
print(data.info())

"""6 Exercise 1.1
3. Write the correlation matrix generated for the following dataset. https://www.kaggle.com/code/
asimislam/titanic-disaster-fe-eda-corr-ml/data?select=train.csv.
"""

import numpy as np
import pandas as pd
import seaborn as sns

data = pd.read_csv('drive/MyDrive/train.csv')
# data.head()
data = data.iloc[:,:-1]
data.head()
corr = data.corr()
print(corr.head())
sns.heatmap(corr)